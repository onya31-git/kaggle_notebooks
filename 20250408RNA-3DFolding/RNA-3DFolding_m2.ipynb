{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanford RNA 3D | RibonanzaNet Explained\n",
    "# https://www.kaggle.com/code/alejopaullier/stanford-rna-3d-ribonanzanet-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本ライブラリ\n",
    "import math\n",
    "import yaml\n",
    "\n",
    "# EDA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# Others\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from functools import partialmethod\n",
    "from typing import Any, Dict, List, Literal,Optional, Tuple, Union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writefile` not found.\n"
     ]
    }
   ],
   "source": [
    "# yamlファイルの作成\n",
    "# %%writefile config.yaml\n",
    "\n",
    "# learning_rate: 0.001  # The learning rate for the optimizer\n",
    "# batch_size: 4  # Number of samples per batch\n",
    "# test_batch_size: 8  # Number of samples per batch\n",
    "# epochs: 30  # Total training epochs\n",
    "# optimizer: \"ranger\"  # Optimization algorithm\n",
    "# dropout: 0.05  # Dropout regularization rate\n",
    "# weight_decay: 0.0001\n",
    "# k: 5\n",
    "# ninp: 256\n",
    "# nlayers: 9\n",
    "# nclass: 2\n",
    "# ntoken: 5  # AUGC + padding/N token\n",
    "# nhead: 8\n",
    "# use_bpp: False\n",
    "# use_flip_aug: true\n",
    "# bpp_file_folder: \"./20250408RNA-3DFolding/data/bpp_files/\"\n",
    "# gradient_accumulation_steps: 1\n",
    "# use_triangular_attention: false\n",
    "# pairwise_dimension: 64\n",
    "# use_bpp: False\n",
    "\n",
    "# #Data scaling\n",
    "# use_data_percentage: 1\n",
    "# use_dirty_data: true  # turn off for data scaling and data dropout experiments\n",
    "\n",
    "# # Other configurations\n",
    "# fold: 0\n",
    "# nfolds: 6\n",
    "# input_dir: \"./20250408RNA-3DFolding/data/\"\n",
    "# gpu_id: \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
      "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
     ]
    }
   ],
   "source": [
    "# ユーティリティ関数\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "        self.entries=entries\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.entries)\n",
    "\n",
    "def default(val: Any, d: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Returns `val` if it is not None, otherwise returns the default value `d`.\n",
    "    :param val: The primary value.\n",
    "    :param d: The default value to return if `val` is None.\n",
    "    :return: `val` if it is not None, otherwise `d`.\n",
    "    \"\"\"\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def exists(val: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether a given value is not None.\n",
    "    :param val: The value to check.\n",
    "    :return: True if `val` is not None, otherwise False.\n",
    "    \"\"\"\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def init_weights(m: torch.nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a given module if it is an instance of `torch.nn.Linear`. \n",
    "    Currently, the function does not apply any initialization but has commented-out \n",
    "    Xavier initialization methods.\n",
    "    :param m: The module to initialize, expected to be a `torch.nn.Linear` instance.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if m is not None and isinstance(m, nn.Linear):\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_config_from_yaml(file_path):\n",
    "    \"\"\"Load YAML file\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return Config(**config)\n",
    "\n",
    "\n",
    "def sep():\n",
    "    print(\"—\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドロップアウト関数\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of dropout with the ability to share the dropout mask\n",
    "    along a particular dimension.\n",
    "\n",
    "    If not in training mode, this module computes the identity function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r: float, batch_dim: Union[int, List[int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            r:\n",
    "                Dropout rate\n",
    "            batch_dim:\n",
    "                Dimension(s) along which the dropout mask is shared\n",
    "        \"\"\"\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        if type(batch_dim) == int:\n",
    "            batch_dim = [batch_dim]\n",
    "        self.batch_dim = batch_dim\n",
    "        self.dropout = nn.Dropout(self.r)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "                Tensor to which dropout is applied. Can have any shape\n",
    "                compatible with self.batch_dim\n",
    "        \"\"\"\n",
    "        shape = list(x.shape)\n",
    "        if self.batch_dim is not None:\n",
    "            for bd in self.batch_dim:\n",
    "                shape[bd] = 1\n",
    "        mask = x.new_ones(shape)\n",
    "        mask = self.dropout(mask)\n",
    "        x = x * mask\n",
    "        return x\n",
    "\n",
    "\n",
    "class DropoutRowwise(Dropout):\n",
    "    \"\"\"\n",
    "    Convenience class for rowwise dropout as described in subsection\n",
    "    1.11.6.\n",
    "    \"\"\"\n",
    "\n",
    "    __init__ = partialmethod(Dropout.__init__, batch_dim=-3)\n",
    "\n",
    "\n",
    "class DropoutColumnwise(Dropout):\n",
    "    \"\"\"\n",
    "    Convenience class for columnwise dropout as described in subsection\n",
    "    1.11.6.\n",
    "    \"\"\"\n",
    "\n",
    "    __init__ = partialmethod(Dropout.__init__, batch_dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mish関数(活性化関数)\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * (torch.tanh(F.softplus(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeMプーリング\n",
    "class GeM(nn.Module):\n",
    "    \"\"\"\n",
    "    1-dimensional GeM pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        kernel_size = (x.size(-1))\n",
    "        output = F.avg_pool1d(\n",
    "            x.clamp(min=self.eps).pow(self.p), \n",
    "            kernel_size\n",
    "        ).pow(1./self.p)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'GeM(p={self.p}, eps={self.eps})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    scaled Dot_Product Attention module, computing attention scores based on query and key similarity.\n",
    "    '''\n",
    "    def __init__(self, temperature: float, attn_dropout: float = 0.1) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Scaled Dot-Product Attention module.\n",
    "        \n",
    "        :param temperature: Scaling factor for the dot product attention scores.\n",
    "        :param atten_dropout: Dropout rate applied to attention weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperatuer: float = temperature\n",
    "        self.dropout: nn.Dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            q: torch.Tensor, # (B, nhead, L, d_q)\n",
    "            k: torch.Tensor, # (B, nhead, L, d_k)\n",
    "            v: torch.Tensor, # (B, nhead, L, d_v)\n",
    "            mask: torch.Tensor | None = None, # (B, 1, L, L) or None\n",
    "            attn_mask: torch.Tensor | None = None #(B, 1, L, L) or None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the Scaled Dot-Product Attention.\n",
    "        \n",
    "        :param q: Query tensor of shape (B, nhead, L, d_k), where B is batch size, nhead is the number of attention heads, L is the sequence length, and d_k is the key/query dimension.\n",
    "        :param k: Key tensor of shape (B, nhead, L, d_k).\n",
    "        :param v: Value tensor of shape (B, nhead, L, d_v), where d_v is the value dimension.\n",
    "        :param mask: Optional bias mask tensor of shape (B, 1, L, L), used for causal masking or padding.\n",
    "        :param attn_mask: Optional attention mask tensor of shape (B, 1, L, L), where -1 values indicate positions to mask.\n",
    "        :return: Tuple containing:\n",
    "            - output (torch.Tensor): The result of the attention mechanism, shape (B, nhead, L, d_v).\n",
    "            - attn (torch.Tensor): Attention weights after softmax and dropout, shape (B, nhead, L, L).\n",
    "        \"\"\"\n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / self.temepature # (B, nhead, L, L)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn + mask\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn = attn.float().masked_fill(attn_mask == -1, float('-1e-9')) # Apply attention mask (B, nhead, L, L)\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = troch.matmul(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module\n",
    "    :param d_model: The number of input features.\n",
    "    :param n_head: The number of heads to use.\n",
    "    :param d_k: The dimensionalyty of the keys.\n",
    "    :param d_v: The dimensionalyty of the values.\n",
    "    :param dropout: The dropout rate to apply to the attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model: int, \n",
    "            n_head: int, \n",
    "            d_k: int, \n",
    "            d_v: int, \n",
    "            dropout: float = 0.1\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.w_qs =nn.Linear(d_model, n_head * d_k, bias=False) # (d_model) -> (n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False) # (d_model) -> (n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False) # (d_model) -> (n_head * d_v)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False) # (n_head * d_v) -> (d_model)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        q: torch.Tensor,  # Shape: [batch_size, len_q, d_model]\n",
    "        k: torch.Tensor,  # Shape: [batch_size, len_k, d_model]\n",
    "        v: torch.Tensor,  # Shape: [batch_size, len_v, d_model]\n",
    "        mask: Optional[torch.Tensor] = None,  # Optional attention mask\n",
    "        src_mask: Optional[torch.Tensor] = None  # Optional source mask\n",
    "               \n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:  # Returns (output, attention)\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        bs, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "\n",
    "        residual = q  # Shape: [bs, len_q, d_model]\n",
    "\n",
    "        # Linear projections and reshape to multiple heads\n",
    "        q = self.w_qs(q).view(bs, len_q, n_head, d_k)  # Shape: [bs, len_q, n_head, d_k]\n",
    "        k = self.w_ks(k).view(bs, len_k, n_head, d_k)  # Shape: [bs, len_k, n_head, d_k]\n",
    "        v = self.w_vs(v).view(bs, len_v, n_head, d_v)  # Shape: [bs, len_v, n_head, d_v]\n",
    "\n",
    "        # Transpose for multi-head attention computation\n",
    "        q, k, v = (\n",
    "            q.transpose(1, 2),  # Shape: [bs, n_head, len_q, d_k]\n",
    "            k.transpose(1, 2),  # Shape: [bs, n_head, len_k, d_k]\n",
    "            v.transpose(1, 2)\n",
    "        )  # Shape: [bs, n_head, len_v, d_v]\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask  # Shape remains unchanged\n",
    "\n",
    "        if src_mask is not None:\n",
    "            src_mask[src_mask == 0] = -1\n",
    "            src_mask = src_mask.unsqueeze(-1).float()  # Shape: [bs, len_k, 1]\n",
    "            attn_mask = torch.matmul(src_mask, src_mask.permute(0, 2, 1)).unsqueeze(1)  \n",
    "            # Shape: [bs, 1, len_k, len_k]\n",
    "            q, attn = self.attention(q, k, v, mask=mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            q, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Reshape back to original format\n",
    "        q = q.transpose(1, 2).contiguous().view(bs, len_q, -1)  # Shape: [bs, len_q, n_head * d_v]\n",
    "        q = self.dropout(self.fc(q))  # Shape: [bs, len_q, d_model]\n",
    "        q += residual  # Shape: [bs, len_q, d_model]\n",
    "\n",
    "        q = self.layer_norm(q)  # Shape: [bs, len_q, d_model]\n",
    "\n",
    "        return q, attn  # Output: [bs, len_q, d_model], Attention: [bs, n_head, len_q, len_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model: int,\n",
    "            dropout: float = 0.1,\n",
    "            max_len: int = 200\n",
    "    ):\n",
    "        super(PositionalEncoding, self).__int__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        position = torch.arange(0,max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, max_len, dtype=torch.float32)) * (-math.log(10000.0 / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param x: tensor of shape (seq_len, batch_size, d_model)\n",
    "        :return: tensor of shape (seq_len, batch_size, d_model) with positional encodings added\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外積平均\n",
    "class OuterProductMean(nn.Module):\n",
    "    \"\"\"\n",
    "    Outer Product Mean class.\n",
    "    :param in_dim: Dimensionality of the input sequence representations (default: 256).\n",
    "    :param dim_msa: Intermediate lower-dimensional representation (default: 32).\n",
    "    :param pairwise_dim: Final dimensionality of the pairwise output (default: 64).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 256,\n",
    "        dim_msa: int = 32,\n",
    "        pairwise_dim: int = 64\n",
    "    ):\n",
    "        super(OuterProductMean, self).__init__()\n",
    "        self.proj_down1 = nn.Linear(dim_msa ** 2, pairwise_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            seq_rep: torch.Tensor,\n",
    "            pair_rep: torch.Tensor = None\n",
    "    ):\n",
    "        seq_rep = self.proj_down1(seq_rep)\n",
    "        outer_product =rearrange(outer_p.roduct, 'b i j c d -> b i j (c d)')\n",
    "        outer_product = self.proj_down2(outer_product)\n",
    "\n",
    "        if pair_rep is not None:\n",
    "            outer_product = outer_product + pair_rep\n",
    "            return outer_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三角形の乗法モジュール\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if val is not None else d\n",
    "\n",
    "class TriangleMultiplicativeModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is applied to the pairwise residue representations, ensuring that the predicted distances \n",
    "    between residues adhere to the triangle inequality principle.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            dim: int, \n",
    "            hidden_dim: Optinoal[int] = None, \n",
    "            mix: str = 'ingoing'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert mix in ('ingoing', 'outgoing'), 'mix must be either ingoing or outgoing'\n",
    "\n",
    "        hidden_dim = default(hidden_dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.left_proj = nn.Linear(dim, hidden_dim)\n",
    "        self.right_proj = nn.Linear(dim, hidden_dim)\n",
    "        self.left_gate = nn.Linear(dim, hidden_dim)\n",
    "        self.right_gate = nn.Linear(dim, hidden_dim)\n",
    "        self.out_gate = nn.Linear(dim, hidden_dim)\n",
    "\n",
    "        # Initialize all gating to identity\n",
    "        for gate in (self.left_gate, self.right_gate, self.out_gate):\n",
    "            nn.init.constant_(gate.weight, 0.)\n",
    "            nn.init.constant_(gate.bias, 1.)\n",
    "\n",
    "        if mix == 'outgoing':\n",
    "            self.mix_einsum_eq = '... i k d, ... j k d -> ... i j d'\n",
    "        elif mix == 'ingoing':\n",
    "            self.mix_einsum_eq = '... k j d, ... k i d -> ... i j d'\n",
    "\n",
    "        self.to_out_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.to_out = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,                  # (batch_size, seq_len, seq_len, dim)\n",
    "        src_mask: Optional[torch.Tensor] = None  # (batch_size, seq_len)\n",
    "    ) -> torch.Tensor:                    # Output: (batch_size, seq_len, seq_len, dim)\n",
    "        if exists(src_mask):\n",
    "            src_mask = src_mask.unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n",
    "            mask = torch.matmul(src_mask, src_mask.permute(0, 2, 1))  # (batch_size, seq_len, seq_len)\n",
    "            mask = rearrange(mask, 'b i j -> b i j ()')  # (batch_size, seq_len, seq_len, 1)\n",
    "\n",
    "        assert x.shape[1] == x.shape[2], 'feature map must be symmetrical'\n",
    "        \n",
    "        x = self.norm(x)  # (batch_size, seq_len, seq_len, dim)\n",
    "\n",
    "        left = self.left_proj(x)  # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        right = self.right_proj(x) # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "\n",
    "        if exists(src_mask):\n",
    "            left = left * mask  # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "            right = right * mask # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "\n",
    "        left_gate = self.left_gate(x).sigmoid()   # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        right_gate = self.right_gate(x).sigmoid() # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        out_gate = self.out_gate(x).sigmoid()     # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "\n",
    "        left = left * left_gate   # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        right = right * right_gate # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "\n",
    "        out = einsum(self.mix_einsum_eq, left, right)  # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "\n",
    "        out = self.to_out_norm(out)  # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        out = out * out_gate         # (batch_size, seq_len, seq_len, hidden_dim)\n",
    "        return self.to_out(out)      # (batch_size, seq_len, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三角形Attention機構\n",
    "class TriangleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 128,\n",
    "        dim: int = 32,\n",
    "        n_heads: int = 4,\n",
    "        wise: Literal['row', 'col'] = 'row'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implements Triangle Attention Mechanism.\n",
    "        :param in_dim: Input feature dimension.\n",
    "        :param dim: Dimension of query, key, and value per head.\n",
    "        :param n_heads: Number of attention heads.\n",
    "        :param wise: Whether to apply row-wise or column-wise attention.\n",
    "        \"\"\"\n",
    "        super(TriangleAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.wise = wise\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.to_qkv = nn.Linear(in_dim, dim * 3 * n_heads, bias=False)\n",
    "        self.linear_for_pair = nn.Linear(in_dim, n_heads, bias=False)\n",
    "        self.to_gate = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.to_out = nn.Linear(n_heads * dim, in_dim)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for TriangleAttention.\n",
    "        :param z: Input tensor of shape (B, I, J, in_dim).\n",
    "        :param src_mask: Source mask of shape (B, I, J).\n",
    "        :return: Output tensor of shape (B, I, J, in_dim).\n",
    "        \"\"\"\n",
    "        # Spawn pair mask\n",
    "        src_mask = src_mask.clone()\n",
    "        src_mask[src_mask == 0] = -1\n",
    "        src_mask = src_mask.unsqueeze(-1).float()  # (B, I, J, 1)\n",
    "        attn_mask = torch.matmul(src_mask, src_mask.permute(0, 2, 1))  # (B, I, J, I)\n",
    "\n",
    "        wise = self.wise\n",
    "        z = self.norm(z)  # (B, I, J, in_dim)\n",
    "\n",
    "        # Compute bias and gate\n",
    "        gate = self.to_gate(z)  # [1] (B, I, J, in_dim)\n",
    "        b = self.linear_for_pair(z)  # [5] (B, I, J, n_heads) \n",
    "\n",
    "        # Compute Q, K, V\n",
    "        q, k, v = torch.chunk(self.to_qkv(z), 3, -1)  # [2], [3], [4]: each (B, I, J, n_heads * dim)\n",
    "        q, k, v = map(lambda x: rearrange(x, 'b i j (h d)->b i j h d', h=self.n_heads), (q, k, v))  \n",
    "        # Each: (B, I, J, n_heads, dim)\n",
    "        scale = q.size(-1) ** 0.5  # Scalar\n",
    "\n",
    "        if wise == 'row':\n",
    "            eq_attn = 'brihd,brjhd->brijh'\n",
    "            eq_multi = 'brijh,brjhd->brihd'\n",
    "            b = rearrange(b, 'b i j (r h)->b r i j h', r=1)  # (B, 1, I, J, n_heads)\n",
    "            softmax_dim = 3\n",
    "            attn_mask = rearrange(attn_mask, 'b i j->b 1 i j 1')  # (B, 1, I, J, 1)\n",
    "        elif wise == 'col':\n",
    "            eq_attn = 'bilhd,bjlhd->bijlh'\n",
    "            eq_multi = 'bijlh,bjlhd->bilhd'\n",
    "            b = rearrange(b, 'b i j (l h)->b i j l h', l=1)  # (B, I, J, 1, n_heads)\n",
    "            softmax_dim = 2\n",
    "            attn_mask = rearrange(attn_mask, 'b i j->b i j 1 1')  # (B, I, J, 1, 1)\n",
    "        else:\n",
    "            raise ValueError('wise should be col or row!')\n",
    "\n",
    "        # Compute attention logits\n",
    "        logits = (torch.einsum(eq_attn, q, k) / scale + b)  # [6], [7] (B, I, J, I, n_heads) or (B, I, J, J, n_heads)\n",
    "        logits = logits.masked_fill(attn_mask == -1, float('-1e-9'))  # Apply mask\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn = logits.softmax(softmax_dim)  # [8] (B, I, J, I, n_heads) or (B, I, J, J, n_heads)\n",
    "\n",
    "        # Compute attention output\n",
    "        out = torch.einsum(eq_multi, attn, v)  # [9] (B, I, J, n_heads, dim)\n",
    "        out = gate * rearrange(out, 'b i j h d-> b i j (h d)')  # [10] (B, I, J, in_dim)\n",
    "\n",
    "        # Final projection\n",
    "        z_ = self.to_out(out)  # (B, I, J, in_dim)\n",
    "\n",
    "        return z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvTransformer エンコーダー\n",
    "class ConvTransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Encoder Layer with convolutional enhancements and pairwise feature processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int,\n",
    "        pairwise_dimension: int,\n",
    "        use_triangular_attention: bool,\n",
    "        dropout: float = 0.1,\n",
    "        k: int = 3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param d_model: Dimension of the input embeddings\n",
    "        :param nhead: Number of attention heads\n",
    "        :param dim_feedforward: Hidden layer size in feedforward network\n",
    "        :param pairwise_dimension: Dimension of pairwise features\n",
    "        :param use_triangular_attention: Whether to use triangular attention modules\n",
    "        :param dropout: Dropout rate\n",
    "        :param k: Kernel size for the 1D convolution\n",
    "        \"\"\"\n",
    "        super(ConvTransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        # === Attention Layers ===\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, d_model // nhead, d_model // nhead, dropout=dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # === Layer Norms ===\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # === Dropout Layers ===\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.pairwise2heads = nn.Linear(pairwise_dimension, nhead, bias=False)\n",
    "        self.pairwise_norm = nn.LayerNorm(pairwise_dimension)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        self.conv = nn.Conv1d(d_model, d_model, k, padding=k // 2)\n",
    "\n",
    "        self.triangle_update_out = TriangleMultiplicativeModule(dim=pairwise_dimension, mix='outgoing')\n",
    "        self.triangle_update_in = TriangleMultiplicativeModule(dim=pairwise_dimension, mix='ingoing')\n",
    "\n",
    "        self.pair_dropout_out = DropoutRowwise(dropout)\n",
    "        self.pair_dropout_in = DropoutRowwise(dropout)\n",
    "\n",
    "        self.use_triangular_attention = use_triangular_attention\n",
    "        if self.use_triangular_attention:\n",
    "            self.triangle_attention_out = TriangleAttention(\n",
    "                in_dim=pairwise_dimension,\n",
    "                dim=pairwise_dimension // 4,\n",
    "                wise='row'\n",
    "            )\n",
    "            self.triangle_attention_in = TriangleAttention(\n",
    "                in_dim=pairwise_dimension,\n",
    "                dim=pairwise_dimension // 4,\n",
    "                wise='col'\n",
    "            )\n",
    "\n",
    "            self.pair_attention_dropout_out = DropoutRowwise(dropout)\n",
    "            self.pair_attention_dropout_in = DropoutColumnwise(dropout)\n",
    "\n",
    "        self.OuterProductMean = OuterProductMean(in_dim=d_model, pairwise_dim=pairwise_dimension)\n",
    "\n",
    "        self.pair_transition = nn.Sequential(\n",
    "            nn.LayerNorm(pairwise_dimension),\n",
    "            nn.Linear(pairwise_dimension, pairwise_dimension * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pairwise_dimension * 4, pairwise_dimension)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,  # Shape: (batch_size, seq_len, d_model)\n",
    "        pairwise_features: torch.Tensor,  # Shape: (batch_size, seq_len, seq_len, pairwise_dimension)\n",
    "        src_mask: torch.Tensor = None,  # Shape: (batch_size, seq_len) or None\n",
    "        return_aw: bool = False\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the ConvTransformerEncoderLayer.\n",
    "\n",
    "        :param src: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        :param pairwise_features: Pairwise feature tensor of shape (batch_size, seq_len, seq_len, pairwise_dimension)\n",
    "        :param src_mask: Optional mask tensor of shape (batch_size, seq_len)\n",
    "        :param return_aw: Whether to return attention weights\n",
    "        :return: Tuple containing processed src and pairwise_features (and optionally attention weights)\n",
    "        \"\"\"\n",
    "        src = src * src_mask.float().unsqueeze(-1)  # Shape: (batch_size, seq_len, d_model)\n",
    "        res = src  # residual\n",
    "        src = src + self.conv(src.permute(0, 2, 1)).permute(0, 2, 1)  # Shape: (batch_size, seq_len, d_model)\n",
    "        src = self.norm3(src)\n",
    "\n",
    "        pairwise_bias = self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0, 3, 1, 2)\n",
    "        src2, attention_weights = self.self_attn(src, src, src, mask=pairwise_bias, src_mask=src_mask)  # Shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))  # Shape: (batch_size, seq_len, d_model)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        pairwise_features = pairwise_features + self.OuterProductMean(src)  # Shape: (batch_size, seq_len, seq_len, pairwise_dimension)\n",
    "        pairwise_features = pairwise_features + self.pair_dropout_out(self.triangle_update_out(pairwise_features, src_mask))\n",
    "        pairwise_features = pairwise_features + self.pair_dropout_in(self.triangle_update_in(pairwise_features, src_mask))\n",
    "        \n",
    "        if self.use_triangular_attention:\n",
    "            pairwise_features = pairwise_features + self.pair_attention_dropout_out(self.triangle_attention_out(pairwise_features, src_mask))\n",
    "            pairwise_features = pairwise_features + self.pair_attention_dropout_in(self.triangle_attention_in(pairwise_features, src_mask))\n",
    "        \n",
    "        pairwise_features = pairwise_features + self.pair_transition(pairwise_features)  # Shape: (batch_size, seq_len, seq_len, pairwise_dimension)\n",
    "\n",
    "        if return_aw:\n",
    "            return src, pairwise_features, attention_weights  # Shapes: (batch_size, seq_len, d_model), (batch_size, seq_len, seq_len, pairwise_dimension), (batch_size, nhead, seq_len, seq_len)\n",
    "        else:\n",
    "            return src, pairwise_features  # Shapes: (batch_size, seq_len, d_model), (batch_size, seq_len, seq_len, pairwise_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相対位置エンコーディング\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements relative positional encoding for sequence-based models.\n",
    "    :param dim: (int) The output embedding dimension. Default is 64.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int = 64):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.linear = nn.Linear(17, dim)  # (17,) -> (dim,)\n",
    "\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the relative positional encodings for a given sequence.\n",
    "\n",
    "        :param src: Input tensor of shape (B, L, D), where:\n",
    "            - B: Batch size\n",
    "            - L: Sequence length\n",
    "            - D: Feature dimension (ignored in this module)\n",
    "        :return: Relative positional encoding of shape (L, L, dim)\n",
    "        \"\"\"\n",
    "        L = src.shape[1]  # Sequence length\n",
    "        res_id = torch.arange(L, device=src.device).unsqueeze(0)  # (1, L)\n",
    "        \n",
    "        device = res_id.device\n",
    "        bin_values = torch.arange(-8, 9, device=device)  # (17,)\n",
    "\n",
    "        d = res_id[:, :, None] - res_id[:, None, :]  # (1, L, L)\n",
    "        bdy = torch.tensor(8, device=device)\n",
    "\n",
    "        # Clipping the values within the range [-8, 8]\n",
    "        d = torch.minimum(torch.maximum(-bdy, d), bdy)  # (1, L, L)\n",
    "\n",
    "        # One-hot encoding of relative positions\n",
    "        d_onehot = (d[..., None] == bin_values).float()  # (1, L, L, 17)\n",
    "\n",
    "        assert d_onehot.sum(dim=-1).min() == 1  # Ensure proper one-hot encoding\n",
    "\n",
    "        # Linear transformation to embedding space\n",
    "        p = self.linear(d_onehot)  # (1, L, L, 17) -> (1, L, L, dim)\n",
    "\n",
    "        return p.squeeze(0)  # (L, L, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RibonanzaNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RibonanzaNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer-based neural network for sequence processing, incorporating convolutional transformer encoder layers,\n",
    "    outer product mean operations, and relative positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: object):\n",
    "        \"\"\"\n",
    "        Initializes the RibonanzaNet model.\n",
    "        \n",
    "        :param config: Configuration object containing model hyperparameters.\n",
    "            - ninp (int): Input embedding dimension.\n",
    "            - ntoken (int): Vocabulary size for embedding layer.\n",
    "            - nclass (int): Number of output classes.\n",
    "            - nhead (int): Number of attention heads.\n",
    "            - nlayers (int): Number of transformer encoder layers.\n",
    "            - dropout (float): Dropout probability.\n",
    "            - pairwise_dimension (int): Dimension of pairwise features.\n",
    "            - use_triangular_attention (bool): Whether to use triangular attention.\n",
    "            - use_bpp (bool): Whether to use base-pairing probability features.\n",
    "            - k (int): Kernel size for convolutions in transformer layers.\n",
    "        \"\"\"\n",
    "        super(RibonanzaNet, self).__init__()\n",
    "        self.config = config\n",
    "        nhid = config.ninp * 4\n",
    "        \n",
    "        self.transformer_encoder = []\n",
    "        print(f\"Constructing {config.nlayers} ConvTransformerEncoderLayers\")\n",
    "        for i in range(config.nlayers):\n",
    "            k = config.k if i != config.nlayers - 1 else 1\n",
    "            self.transformer_encoder.append(\n",
    "                ConvTransformerEncoderLayer(\n",
    "                    d_model=config.ninp, nhead=config.nhead,\n",
    "                    dim_feedforward=nhid,\n",
    "                    pairwise_dimension=config.pairwise_dimension,\n",
    "                    use_triangular_attention=config.use_triangular_attention,\n",
    "                    dropout=config.dropout, k=k)\n",
    "            )\n",
    "        self.transformer_encoder = nn.ModuleList(self.transformer_encoder)\n",
    "        \n",
    "        self.encoder = nn.Embedding(config.ntoken, config.ninp, padding_idx=4)\n",
    "        self.decoder = nn.Linear(config.ninp, config.nclass)\n",
    "        \n",
    "        if config.use_bpp:\n",
    "            self.mask_dense = nn.Conv2d(2, config.nhead // 4, 1)\n",
    "        else:\n",
    "            self.mask_dense = nn.Conv2d(1, config.nhead // 4, 1)\n",
    "        \n",
    "        self.OuterProductMean = OuterProductMean(in_dim=config.ninp, pairwise_dim=config.pairwise_dimension)\n",
    "        self.pos_encoder = RelativePositionalEncoding(config.pairwise_dimension)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None, return_aw: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the RibonanzaNet model.\n",
    "        \n",
    "        :param src: Input tensor of shape (B, L), where B is the batch size and L is the sequence length.\n",
    "        :param src_mask: Optional mask tensor of shape (B, L, L), used for attention masking.\n",
    "        :param return_aw: Boolean flag indicating whether to return attention weights.\n",
    "        :return: Output tensor of shape (B, L, nclass) if return_aw is False, or a tuple (output, attention_weights).\n",
    "        \"\"\"\n",
    "        B, L = src.shape  # (Batch size, Sequence length)\n",
    "        src = self.encoder(src).reshape(B, L, -1)  # (B, L, ninp)\n",
    "        \n",
    "        pairwise_features = self.OuterProductMean(src)  # (B, L, L, pairwise_dimension)\n",
    "        pairwise_features = pairwise_features + self.pos_encoder(src)  # (B, L, L, pairwise_dimension)\n",
    "        \n",
    "        attention_weights = []\n",
    "        for i, layer in enumerate(self.transformer_encoder):\n",
    "            if src_mask is not None:\n",
    "                if return_aw:\n",
    "                    src, aw = layer(src, pairwise_features, src_mask, return_aw=return_aw)\n",
    "                    attention_weights.append(aw)\n",
    "                else:\n",
    "                    src, pairwise_features = layer(src, pairwise_features, src_mask, return_aw=return_aw)\n",
    "            else:\n",
    "                if return_aw:\n",
    "                    src, aw = layer(src, pairwise_features, return_aw=return_aw)\n",
    "                    attention_weights.append(aw)\n",
    "                else:\n",
    "                    src, pairwise_features = layer(src, pairwise_features, return_aw=return_aw)\n",
    "        \n",
    "        output = self.decoder(src).squeeze(-1) + pairwise_features.mean() * 0  # (B, L, nclass)\n",
    "        \n",
    "        if return_aw:\n",
    "            return output, attention_weights\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "config = load_config_from_yaml(\"config.yaml\")\n",
    "model = RibonanzaNet(config).cuda()\n",
    "x = torch.ones(4, 128).long().cuda()\n",
    "mask = torch.ones(4, 128).long().cuda()\n",
    "mask[:,120:] = 0\n",
    "print(f\"Output shape: {model(x,src_mask=mask).shape}\"), sep()\n",
    "modelq "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
